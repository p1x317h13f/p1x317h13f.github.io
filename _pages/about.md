---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

My name is Krystal, I am currently working as a Non-Resident Research Fellow at [UC Berkeley’s Center for Long-Term Cybersecurity, AI Security Initiative](https://cltc.berkeley.edu/program/ai-security-initiative/), and a Co-Founder and Research Director of [Black in AI Safety and Ethics](https://www.baseresearch.org/). 

Overview
------
My research focuses on developing models and analytical frameworks to assess the risks associated with emerging technologies, capturing the interplay between system capabilities and sociotechnical contexts. This work bridges traditional governance, risk, and compliance (GRC), information security, predictive analytics and forecasting, data science, ethics, and normative theory. I lead efforts to develop practical tools, methodologies, and evidence-based policy recommendations to address the challenges of governing advanced artificial intelligence (AI) in an increasingly complex cybersocial landscape.

Past Work
------
I’m interested in areas where theory meets practice, and there is an opportunity to turn weak signals around emerging risks into novel research and ultimately proactive mitigation actions. As a result, my experiences have centered on bridging the gap between technical research and real-world best practices, standards, and regulations. 

During my tenure as a Non-Resident Research Fellow at UC Berkeley’s Center for Long-Term Cybersecurity, AI Security Initiative, I have co-authored four reports on risk management for general-purpose/frontier AI systems. This includes the only non-governmental companion resource for the NIST AI Risk Management Framework (AI RMF) [officially cited by NIST](https://www.nist.gov/itl/ai-risk-management-framework/ai-risk-management-framework-resources)[^1]. In this role, I have also contributed to standards-setting initiatives, such as the INCITS Artificial Intelligence Committee, which serves as the U.S. TAG to ISO/IEC JTC 1/SC 42,[^2] [NIST’s U.S. AI Safety Institute Consortium](https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute-consortium-aisic), and the [Zero Drafts project](https://www.nist.gov/artificial-intelligence/ai-research/nists-ai-standards-zero-drafts-pilot-project-accelerate).[^3] In this role I’ve also provided research and feedback on the [EU General-Purpose AI Code of Practice](https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai) and international initiatives such as the [Paris Peace Forum Call for Trust and Security in Cyberspace](https://parispeaceforum.org/publications/forging-global-cooperation-on-ai-risks-cyber-policy-as-a-governance-blueprint/). 

In my prior work as a research associate, I helped establish consensus on security best practices across leading frontier AI model developers and deployers.[^4] To do this, I coordinated with information security experts, from CISOs to practitioners, to align on common threat models. During my tenure as an IT Cybersecurity Specialist and AI Capabilities Analyst at the Cybersecurity and Infrastructure Security Agency (CISA), I led AI policy initiatives for the Infrastructure Security Division. While there, I developed methods for scoping and executing AI projects across our mission space and created an ethical analysis framework for agency-wide assessment of AI use. 

My interest in AI and cyber policy was sparked at the Center for Security and Emerging Technology, where, as a Junior AI Fellow, I conducted research on the current state and future prospects of autonomous cyber defense. This is also when I began constructing mathematical models to predict AI’s impact on cybersecurity. I have been interested in understanding AI-enabled scams, fraud, and social engineering in particular, and continue to write about these risks since originally exploring them as the topic of [my master’s thesis](https://arxiv.org/abs/2310.06998v1). 

Outside of these primary roles, I have also served as an independent expert consultant for government agencies and Fortune 500 companies, advising senior decision makers in the US, UK, and EU on emerging AI threats and technological guardrails.[^5] I regularly serve as a mentor and educator and have had the pleasure of working with the Cambridge ERA AI Fellowship, BlueDot Impact, the Center for AI and Digital Policy (CAIDP), and, most recently, Black in AI Safety and Ethics (BASE). My service to the field includes serving as a Trusted AI Safety Expert with the Cloud Security Alliance and a Subject Matter Expert with Hack The Box.

I approach technology policy with the mindset of a community organizer. My advocacy work ranges from grassroots efforts, such as launching a chapter of Data for Black Lives and successfully campaigning to ban facial recognition locally, to global governance, such as shaping federal guidelines for NIST, NTIA, and OSTP, and joining Nobel Laureates as a signatory on the European Commission letter, [*Ensuring GPAI Rules Serve the Interests of European Businesses and Citizens*](https://allai.nl/wp-content/uploads/2025/06/ProtectingGPAIRules.pdf). 

My work has been received at a variety of venues, including the RSA Conference, Stanford’s Trust and Safety Research Conference, TrustCon, INFORMS, and AAAI. Although my research spans multiple domains, it is primarily applicable to risk management and analysis, supporting policy recommendations. 

Overview of Risk Management Research
------
For over two years, I have contributed to the [*AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models*](https://arxiv.org/abs/2506.23949). This framework adapts general AI risk management standards, specifically the NIST AI RMF and ISO/IEC 23894, to the unique challenges of managing the risks posed by General-Purpose AI (GPAI) and frontier/foundation models. The goal of the Profile is to provide developers with actionable guidance to identify and mitigate risks typical of these multi-purpose systems, such as emergent behaviors and large-scale misuse. We have built on this foundational work by releasing additional resources, including [*Mapping of Profile Guidance V1.1 to Key Standards and Regulations*](https://cltc.berkeley.edu/wp-content/uploads/2025/01/Berkeley-Mapping-of-Profile-Guidance-v1-1-to-Key-Standards-and-Regulations.pdf), [*Retrospective Test Use of Profile Guidance*](https://cltc.berkeley.edu/wp-content/uploads/2025/01/Berkeley-Retrospective-Test-Use-of-Profile-v1-1.pdf) (a document demonstrating the feasibility of our guidance), and a [*Profile Quick Guide*](https://cltc.berkeley.edu/wp-content/uploads/2025/01/Berkeley-Profile-v1-1-Quick-Guide.pdf) (a short introductory resource designed to complement the full profile).

I have also worked to create concrete recommendations for defining and operationalizing [*Intolerable Risk Threshold Recommendations for Artificial Intelligence*](https://arxiv.org/abs/2503.05812)—specific thresholds that, if crossed by an AI system, represent a severe threat to public safety or societal stability. This work aims to move the field beyond vague voluntary commitments toward specific, measurable red lines that can guide both industry safety frameworks and government regulation. The Center for Long-Term Cybersecurity has joined as a partner in the global call for [AI red lines](https://red-lines.ai/) and continues to be a leading voice in convening discussions and research in this space. 

To advance the science of AI system evaluation, we proposed [*Benchmark Early and Red Team Often: A Framework for Assessing and Managing Dual-Use Hazards of AI Foundation Models*](https://arxiv.org/abs/2405.10986), a cost-effective and rigorous framework for identifying foundation models that possess dangerous dual-use capabilities, specifically those that could aid in chemical, biological, radiological, and nuclear (CBRN) or cyber attacks. This work seeks to bridge the gap between two existing evaluation methods: open benchmarks, which are cheap but can suffer from specification gaming and overfitting, and red-team evaluations, which can be more accurate but also more expensive and slower. To strike a balance and improve evaluation methods, we introduced the Benchmark and Red Team AI Capability Evaluation (BRACE) Framework. This approach advocates using low-cost benchmarks frequently during development to trigger more expensive red-teaming only when specific score thresholds are crossed.

We will increasingly need to predict when and how AI systems will pose a threat. To ground policy debates in more rigorous analysis, I analyzed a few future development scenarios. In [*Will AI Make Cyber Swords or Shields?*](https://arxiv.org/abs/2207.13825), my co-author and I used mathematical models to quantify the impact of AI on critical dynamics, including phishing, vulnerability discovery, and patching. This research challenged prevailing assumptions, demonstrating mathematically that while AI aids attackers in phishing, improvements in automated patch deployment could vastly outweigh advances in automated exploit generation.

[^1]: It is also one of the few risk management resources cited by the [U.S. State Department](https://2021-2025.state.gov/risk-management-profile-for-ai-and-human-rights/) and has received the endorsement of policy representatives at leading AI labs, such as [Google DeepMind](https://cltc.berkeley.edu/publication/ai-risk-management-standards-profile/). 
[^2]: Participation via the Berkeley Existential Risk Initiative. SC 42 is involved in developing several different standards that cover various aspects of AI technology, including frameworks for AI systems, evaluation methods, and ethical considerations.
[^3]: Participation via the University of California Berkeley.
[^4]: Member organizations at the time included Meta, Microsoft, Google, Anthropic, OpenAI, and Amazon. Example: [Foundational Security Practices (2024)](https://www.frontiermodelforum.org/updates/issue-brief-foundational-security-practices/).
[^5]: As a consultant with Duco Experts, Dialectica, and others.

Select Publications 
------
- [*Survey of Search Engine Safeguards and their Applicability for AI*](https://cltc.berkeley.edu/publication/survey-of-search-engine-safeguards-and-their-applicability-for-ai/)
- [*Intolerable Risk Threshold Recommendations for Artificial Intelligence*](https://arxiv.org/abs/2503.05812)
- [*Benchmark Early and Red Team Often: A Framework for Assessing and Managing Dual-Use Hazards of AI Foundation Models*](https://arxiv.org/abs/2405.10986)
- [*AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models*](https://arxiv.org/abs/2506.23949)
- [*Creating Auditing Tools for AI Equity*](https://fas.org/publication/creating-auditing-tools-for-ai-equity/)
- [*Autonomous Cyber Defense: A Roadmap from Lab to Ops*](https://cset.georgetown.edu/publication/autonomous-cyber-defense/)
- [*Will AI Make Cyber Swords or Shields?*](https://arxiv.org/abs/2207.13825) [*(policy report version)*](https://cset.georgetown.edu/publication/will-ai-make-cyber-swords-or-shields/) 

